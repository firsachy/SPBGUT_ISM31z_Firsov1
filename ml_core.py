import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics.pairwise import cosine_distances, euclidean_distances
import tensorflow as tf
from tensorflow import keras
import pickle
import os

class HybridMLCore:
    def __init__(self):
        self.feature_extractor = None
        self.clusterer = None
        self.clusters = []  # –¢–µ–∫—É—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –≤–µ—Å–∞–º–∏
        self.is_trained = False
        self.cluster_metric = 'cosine'  # –ú–µ—Ç—Ä–∏–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def create_feature_extractor(self, architecture, embedding_size):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        try:
            if architecture == "–ú–∞–ª–µ–Ω—å–∫–∏–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω":
                model = keras.Sequential([
                    keras.layers.Flatten(input_shape=(28, 28)),
                    keras.layers.Dense(128, activation='relu'),
                    keras.layers.Dense(64, activation='relu'),
                    keras.layers.Dense(embedding_size, activation='relu', name='embedding')
                ])
            elif architecture == "–ü—Ä–æ—Å—Ç–∞—è CNN":
                model = keras.Sequential([
                    keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),
                    keras.layers.Conv2D(32, (3, 3), activation='relu'),
                    keras.layers.MaxPooling2D((2, 2)),
                    keras.layers.Conv2D(64, (3, 3), activation='relu'),
                    keras.layers.MaxPooling2D((2, 2)),
                    keras.layers.Flatten(),
                    keras.layers.Dense(embedding_size, activation='relu', name='embedding')
                ])
            elif architecture == "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é CNN –∫–∞–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é
                model = keras.Sequential([
                    keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),
                    keras.layers.Conv2D(16, (3, 3), activation='relu'),
                    keras.layers.MaxPooling2D((2, 2)),
                    keras.layers.Flatten(),
                    keras.layers.Dense(embedding_size, activation='relu', name='embedding')
                ])
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {architecture}")
            
            self.feature_extractor = model
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä: {architecture}, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding_size}")
            return model
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞: {e}")
            raise
    
    def train_feature_extractor(self, training_data, epochs=5):
        """–û–±—É—á–∞–µ—Ç —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if self.feature_extractor is None:
                raise ValueError("–°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä!")
            
            x_train = training_data['x_train']
            y_train = training_data['y_train']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º—É –¥–∞–Ω–Ω—ã—Ö
            if len(x_train.shape) == 3:  # (samples, 28, 28)
                x_train = x_train.astype('float32')
            else:
                raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {x_train.shape}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_model = keras.Sequential([
                self.feature_extractor,
                keras.layers.Dense(64, activation='relu'),
                keras.layers.Dense(10, activation='softmax')
            ])
            
            training_model.compile(
                optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            print(f"üéØ –û–±—É—á–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–∞ {len(x_train)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
            
            # –û–±—É—á–∞–µ–º
            history = training_model.fit(
                x_train, y_train,
                epochs=epochs,
                batch_size=32,
                validation_split=0.2,
                verbose=1
            )
            
            self.is_trained = True
            final_accuracy = history.history['accuracy'][-1]
            print(f"‚úÖ –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –æ–±—É—á–µ–Ω! –¢–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.3f}")
            
            return history.history
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞: {e}")
            raise
    
    def extract_features(self, images):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        if not self.is_trained:
            raise ValueError("–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω!")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        if len(images.shape) == 3:
            images = images.astype('float32')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = self.feature_extractor.predict(images, verbose=0)
        return features
    
    def perform_clustering(self, features, clustering_config):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        try:
            algorithm = clustering_config['algorithm']
            self.cluster_metric = clustering_config.get('metric', 'cosine')
            
            print(f"üìä –ó–∞–ø—É—Å–∫–∞–µ–º {algorithm} –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...")
            
            if algorithm == "K-Means":
                n_clusters = clustering_config.get('k_value', 15)
                self.clusterer = KMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    n_init=10
                )
                print(f"   K-Means —Å {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏")
                
            elif algorithm == "DBSCAN":
                eps = clustering_config.get('eps_value', 0.8)
                min_samples = clustering_config.get('min_samples', 5)
                self.clusterer = DBSCAN(
                    eps=eps,
                    min_samples=min_samples,
                    metric=self.cluster_metric
                )
                print(f"   DBSCAN —Å eps={eps}, min_samples={min_samples}")
                
            elif algorithm == "Mean Shift":
                from sklearn.cluster import MeanShift
                bandwidth = clustering_config.get('bandwidth', 0.5)
                self.clusterer = MeanShift(bandwidth=bandwidth)
                print(f"   Mean Shift —Å bandwidth={bandwidth}")
                
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º: {algorithm}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
            cluster_labels = self.clusterer.fit_predict(features)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            self._initialize_clusters(features, cluster_labels, clustering_config)
            
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –°–æ–∑–¥–∞–Ω–æ {len(self.clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            return cluster_labels
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            # –°–æ–∑–¥–∞–µ–º fallback –∫–ª–∞—Å—Ç–µ—Ä—ã
            self._create_fallback_clusters(features, clustering_config)
            return np.zeros(len(features))  # –í—Å–µ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
    
    def _initialize_clusters(self, features, cluster_labels, params):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏"""
        self.clusters = []
        unique_clusters = np.unique(cluster_labels)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è –∫–æ–≥–¥–∞ –Ω–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (DBSCAN)
        if len(unique_clusters) == 1 and unique_clusters[0] == -1:
            print("‚ö†Ô∏è  DBSCAN –Ω–µ –Ω–∞—à–µ–ª –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä.")
            self._create_fallback_clusters(features, params)
            return
        
        # –£–±–∏—Ä–∞–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏ (-1)
        unique_clusters = unique_clusters[unique_clusters != -1]
        
        if len(unique_clusters) == 0:
            print("‚ö†Ô∏è  –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä.")
            self._create_fallback_clusters(features, params)
            return
        
        for cluster_id in unique_clusters:
            # –ù–∞—Ö–æ–¥–∏–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_mask = cluster_labels == cluster_id
            cluster_points = features[cluster_mask]
            
            if len(cluster_points) == 0:
                continue
                
            centroid = np.mean(cluster_points, axis=0)
            
            # –ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            weights = {digit: 1.0/10 for digit in range(10)}
            
            cluster_data = {
                'cluster_id': int(cluster_id),
                'centroid': centroid,
                'weights': weights,
                'params': params,
                'size': len(cluster_points)
            }
            
            self.clusters.append(cluster_data)
            print(f"   –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {len(cluster_points)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def _create_fallback_clusters(self, features, params):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∫–æ–≥–¥–∞ –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–µ –Ω–∞—à–µ–ª –∫–ª–∞—Å—Ç–µ—Ä—ã"""
        print("üîÑ –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã...")
        
        # –ü—Ä–æ—Å—Ç–æ–π K-Means –∫–∞–∫ fallback
        from sklearn.cluster import KMeans
        n_clusters = min(10, len(features))
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=5)
        cluster_labels = kmeans.fit_predict(features)
        
        self.clusterer = kmeans
        self._initialize_clusters(features, cluster_labels, params)
    
    def find_nearest_cluster(self, features):
        """–ù–∞—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.clusters:
            raise ValueError("–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
        
        # –î–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        if len(features.shape) == 1:
            features = features.reshape(1, -1)
        
        nearest_cluster_id = None
        min_distance = float('inf')
        
        for cluster in self.clusters:
            if self.cluster_metric == 'cosine':
                dist = cosine_distances(features, [cluster['centroid']])[0][0]
            else:  # euclidean
                dist = np.linalg.norm(features - cluster['centroid'])
            
            if dist < min_distance:
                min_distance = dist
                nearest_cluster_id = cluster['cluster_id']
        
        return nearest_cluster_id, min_distance
    
    def predict(self, image):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ü–∏—Ñ—Ä—É –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.extract_features(np.array([image]))
            features = features[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π (–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π) –ø—Ä–∏–º–µ—Ä
            
            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä
            cluster_id, distance = self.find_nearest_cluster(features)
            cluster = self.get_cluster_by_id(cluster_id)
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ü–∏—Ñ—Ä—É —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –≤–µ—Å–æ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            predicted_digit = max(cluster['weights'].items(), key=lambda x: x[1])[0]
            confidence = cluster['weights'][predicted_digit]
            
            return predicted_digit, confidence, cluster_id, features
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return 0, 0.1, 0, None  # Fallback prediction
    
    def get_cluster_by_id(self, cluster_id):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä –ø–æ ID"""
        for cluster in self.clusters:
            if cluster['cluster_id'] == cluster_id:
                return cluster
        raise ValueError(f"–ö–ª–∞—Å—Ç–µ—Ä —Å ID {cluster_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    def update_cluster_weights(self, cluster_id, user_feedback, true_label=None, 
                             alpha=0.2, beta=0.5, gamma=0.5, min_weight=0.05):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        try:
            cluster = self.get_cluster_by_id(cluster_id)
            
            if user_feedback == 'yes':
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π —Ü–∏—Ñ—Ä—ã
                predicted_digit = max(cluster['weights'].items(), key=lambda x: x[1])[0]
                old_weight = cluster['weights'][predicted_digit]
                cluster['weights'][predicted_digit] = old_weight + alpha * (1 - old_weight)
                print(f"‚úÖ –£–≤–µ–ª–∏—á–∏–ª–∏ –≤–µ—Å —Ü–∏—Ñ—Ä—ã {predicted_digit} –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ {cluster_id}")
                
            elif user_feedback == 'no':
                # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π —Ü–∏—Ñ—Ä—ã
                predicted_digit = max(cluster['weights'].items(), key=lambda x: x[1])[0]
                cluster['weights'][predicted_digit] *= gamma
                print(f"‚úÖ –£–º–µ–Ω—å—à–∏–ª–∏ –≤–µ—Å —Ü–∏—Ñ—Ä—ã {predicted_digit} –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ {cluster_id}")
                
            elif user_feedback == 'verified' and true_label is not None:
                # –°–∏–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
                old_weight = cluster['weights'][true_label]
                cluster['weights'][true_label] = old_weight + beta * (1 - old_weight)
                print(f"‚úÖ –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è: —É—Å–∏–ª–∏–ª–∏ –≤–µ—Å —Ü–∏—Ñ—Ä—ã {true_label} –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ {cluster_id}")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
            self._normalize_weights(cluster, min_weight)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤: {e}")
    
    def _normalize_weights(self, cluster, min_weight):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ—Å–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥"""
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        for digit in cluster['weights']:
            cluster['weights'][digit] = max(cluster['weights'][digit], min_weight)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á—Ç–æ–±—ã —Å—É–º–º–∞ –±—ã–ª–∞ = 1
        total = sum(cluster['weights'].values())
        for digit in cluster['weights']:
            cluster['weights'][digit] /= total
    
    def get_clusters_data_for_db(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy arrays –≤ —Å–ø–∏—Å–∫–∏ –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        clusters_data = []
        for cluster in self.clusters:
            cluster_copy = cluster.copy()
            cluster_copy['centroid'] = cluster['centroid'].tolist()  # numpy -> list
            clusters_data.append(cluster_copy)
        
        return clusters_data
    
    def load_clusters_from_db(self, clusters_data):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ë–î"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ numpy arrays
        self.clusters = []
        for cluster in clusters_data:
            cluster_copy = cluster.copy()
            cluster_copy['centroid'] = np.array(cluster['centroid'])  # list -> numpy
            self.clusters.append(cluster_copy)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ –ë–î")
    
    def get_system_info(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ"""
        return {
            'is_trained': self.is_trained,
            'clusters_count': len(self.clusters),
            'extractor_architecture': self.feature_extractor._name if self.feature_extractor else None,
            'clusterer_type': type(self.clusterer).__name__ if self.clusterer else None
        }
    
    def save_models(self, filepath='models/hybrid_system'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            os.makedirs('models', exist_ok=True)
            
            if self.feature_extractor:
                self.feature_extractor.save(f'{filepath}_extractor.h5')
            if self.clusterer:
                with open(f'{filepath}_clusterer.pkl', 'wb') as f:
                    pickle.dump(self.clusterer, f)
            
            print("‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
    
    def load_models(self, filepath='models/hybrid_system'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            if os.path.exists(f'{filepath}_extractor.h5'):
                self.feature_extractor = keras.models.load_model(f'{filepath}_extractor.h5')
            if os.path.exists(f'{filepath}_clusterer.pkl'):
                with open(f'{filepath}_clusterer.pkl', 'rb') as f:
                    self.clusterer = pickle.load(f)
            
            self.is_trained = True
            print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")